# AI 编程的常识与认知

> 理解 AI 编程助手的能力边界，才能更好地驾驭它。

---

## 一、陷阱：AI 编程的常见问题

> 如果你在使用 AI 编程时遇到困惑，这里列出了最常见的问题。理解这些陷阱，是有效使用 AI 的第一步。

以下问题按因果关系排序：根源问题 → 模型固有问题 → 实现质量问题 → 专项风险。

### 1. 上下文丢失（根源问题）

长对话或大文件中，AI 会「遗忘」早期信息，导致前后不一致。**这是许多其他问题的根本原因。**

**典型表现：**
- 前后矛盾的实现方式
- 重复询问已提供的信息
- 忽略之前约定的规范
- 在长文件中丢失关键约束

**应对策略：**
- 建立项目级规范文档（`RULES.md`），供 AI 持续引用
- 定期在对话中重申重要约束
- 将长任务拆分为独立的短会话
- 使用 OpenSpec 等规格驱动框架持久化需求（详见第四节）

---

### 2. 缺乏项目全局观

AI 只能看到当前上下文，缺乏对整个项目架构的理解。

**典型表现：**
- 重复生成功能函数：不同模块中出现多个 `计算MD5`、`str2wstr` 等工具函数
- 忽视已有的工具类/公共模块，重复造轮子
- 不了解项目中已存在的依赖库，重新实现已有功能

**应对策略：**
- 建立项目级的代码规范文档（`RULES.md`）
- 明确告知 AI 项目中已有的公共模块位置
- 在提示词中引用相关的工具类文件

---

### 3. 幻觉与虚构代码（模型固有问题）

AI 可能生成看起来合理但实际不存在的 API、库或方法。

**典型表现：**
- 虚构的第三方库函数名
- 不存在的 API 参数或方法
- 编造的配置选项
- 混淆不同版本的 API

**应对策略：**
- 对陌生的 API 调用进行验证
- 查阅官方文档确认方法存在
- 运行代码验证而非仅靠阅读

---

### 4. 过时的知识（模型固有问题）

AI 的训练数据有截止日期，可能推荐已被弃用的方案。

**典型表现：**
- 使用已废弃的 API
- 推荐过时的依赖版本
- 采用不再推荐的设计模式
- 忽略新版本的更优方案

**应对策略：**
- 检查依赖库的最新版本
- 对关键技术选型查阅最新文档
- 使用联网搜索功能获取最新信息

---

### 5. 缺乏实现的一致性

AI 不会主动维护代码风格和实现方式的一致性，每次生成可能采用不同的方案。

**典型表现：**
- 在有 log 系统的情况下，直接使用 `print` 输出
- 在有统一错误处理机制时，使用裸露的 `try-catch`
- 在有配置中心时，硬编码配置值
- 命名风格不统一：驼峰、下划线混用

**应对策略：**
- 提供清晰的代码规范示例
- 要求 AI 参考项目中已有的相似实现
- 代码审查时重点关注一致性问题

---

### 6. 功能实现的自由发挥

在缺少明确规范的情况下，AI 会根据「最佳实践」或「训练数据中的常见模式」自由选择实现方式。

**典型表现：**
- 递归 vs 循环：AI 可能随意选择
- 同步 vs 异步：没有明确要求时随机决定
- 数据结构选择：`List` vs `Set` vs `Map` 的选择缺乏考量
- 算法复杂度：可能选择简单但低效的实现

**应对策略：**
- 在需求中明确指定实现方式
- 提供性能和内存的约束条件
- 要求 AI 解释选择某种实现的理由

---

### 7. 过度工程化

AI 倾向于添加不必要的复杂性，追求「最佳实践」而忽略实际需求。

**典型表现：**
- 滥用设计模式
- 过度抽象和分层
- 引入不必要的依赖
- 简单问题复杂化

**应对策略：**
- 明确告知项目规模和复杂度要求
- 要求 AI 解释每个抽象层的必要性
- 遵循 KISS 原则（Keep It Simple, Stupid）

---

### 8. Agent 执行不完整（Agent 行为问题）

使用 Agent 模式开发时，AI 可能在任务完成前提前停止，或自行跳过某些步骤。

**典型表现：**
- 执行完一两个步骤后就宣布「完成」，后续步骤被忽略
- 跳过 Agent 认为「不重要」的步骤（如单测、文档更新、代码清理）
- 遇到困难或报错时放弃当前路径，而非尝试解决
- 任务列表中的后续项被默默忽略，不主动汇报进度
- 长任务中途「偷懒」，只完成最容易的部分

**根本原因：**
- Agent 的行为受 token 预算和生成策略影响，倾向于尽快结束
- 模型对「完成」的判断标准与人类不同，可能认为部分完成即可
- 缺乏持久化的任务追踪机制，容易「忘记」待办事项

**应对策略：**
- 使用明确的任务清单，要求 AI 逐项确认完成状态
- 设置验收标准（Acceptance Criteria），要求 AI 自检是否满足
- 对于复杂任务，分阶段检查进度，每个阶段验收后再继续
- 在提示词中强调「必须完成所有步骤」「不要跳过任何步骤」
- 使用 OpenSpec 等框架持久化任务列表，避免遗忘

---

### 9. 安全性盲区（专项风险）

AI 可能忽略安全最佳实践，生成存在漏洞的代码。

**典型表现：**
- SQL 拼接而非参数化查询
- 硬编码敏感信息（密钥、密码）
- 缺少输入验证和过滤
- 忽略权限检查

**应对策略：**
- 使用安全扫描工具审查生成代码
- 在提示词中明确安全要求
- 建立安全编码检查清单

---

## 二、本质：理解 AI 的工作原理

> 理解了陷阱之后，我们来看看这些问题的根本原因——AI 到底是如何工作的。

### 工作原理：下一个 Token 预测

AI 编程助手的核心是**大语言模型（LLM）**，其工作原理是：

```
输入序列 → [预测下一个最可能的 token] → 输出序列
                    ↓
            重复直到完成
```

**关键理解：**
- AI 不是在「思考」或「推理」，而是在做**统计性的模式匹配**
- 它从海量代码中学习了「什么代码通常跟在什么代码后面」
- 每次生成都是概率采样，相同输入可能产生不同输出

### 形象比喻：高级补全引擎

可以把 AI 理解为一个**超级智能的代码补全工具**：

```
传统补全：print( → print()
AI 补全：  "读取文件并写入数据库" → [完整的实现代码]
```

| 转换类型 | 示例 |
|----------|------|
| 自然语言 → 代码 | "计算列表平均值" → `sum(lst) / len(lst)` |
| 代码 → 代码 | C++ 实现 → Python 实现 |
| 代码 → 文档 | 函数代码 → 注释和文档 |
| 风格转换 | 同步代码 → 异步代码 |

### 核心机制

| 机制 | 解释 | 影响 |
|------|------|------|
| **上下文窗口** | AI 能「看到」的信息量有限（如 128K tokens） | 超长文件/对话会导致早期信息被截断 |
| **自回归生成** | 逐 token 生成，已生成的内容影响后续生成 | 早期错误会「滚雪球」，越错越远 |
| **概率采样** | 从多个可能的 token 中按概率选择 | 相同提示可能产生不同结果 |
| **训练数据依赖** | 能力来自训练数据中的模式 | 罕见场景表现差，常见场景表现好 |

### 能力边界

| 能力 | 说明 |
|------|------|
| ✅ 理解上下文中的 API | 基于提供的文档/示例理解接口用法 |
| ✅ 语言间改写 | 将 C++ 代码改写为 Python、Java 等 |
| ✅ 风格转换 | 面向对象 ↔ 函数式、同步 ↔ 异步 |
| ✅ 常见模式实现 | 训练数据中高频出现的代码模式 |
| ❌ 原创性突破 | 无法发明训练数据中不存在的算法 |
| ❌ 精确计算 | 数学运算、计数等可能出错 |
| ❌ 深度业务理解 | 需要人类提供业务背景和约束 |

### 核心特征

| 特征 | 解释 |
|------|------|
| **会话内有记忆** | 单次对话中能记住之前的内容 |
| **跨会话无状态** | 新对话不记得之前的交流 |
| **无全局观** | 只能看到提供的上下文，无法主动探索项目 |
| **知识截止** | 训练数据有时间边界，不了解最新技术 |

### 为什么会产生「幻觉」

AI 编造不存在的 API 或库，根本原因是：

1. **必须给出回答**：模型被训练为总是生成内容，不会说「我不知道」
2. **概率填充**：当缺乏准确信息时，用统计上「看起来合理」的内容填充
3. **模式泛化**：将学到的模式错误地应用到不适用的场景

```
用户：PyTorch 中如何使用 model.fast_forward() ?
AI：（训练数据中没见过这个方法，但 fast_forward 看起来合理）
    → 编造一个看似合理但不存在的用法
```

---

## 三、擅长：AI 编程的优势领域

> 了解了 AI 的工作原理后，我们来看看它在哪些场景下表现出色。

### 1. 功能函数与功能模块

AI 在编写边界清晰、输入输出明确的功能代码时表现出色：

- **工具函数**：字符串处理、日期格式化、数据转换
- **CRUD 操作**：标准的数据库增删改查
- **API 封装**：对第三方接口的调用封装
- **数据校验**：表单验证、参数检查

---

### 2. 模式匹配与模仿

AI 本质上是强大的模式匹配引擎，擅长「照葫芦画瓢」：

| 场景 | AI 表现 |
|------|---------|
| 按照已有代码风格编写新功能 | ✅✅✅ 优秀 |
| 将设计文档转化为代码 | ✅✅ 良好 |
| 全新架构设计 | ⚠️ 一般 |
| 创新性算法发明 | ❌ 困难 |

**关键洞察：**
- **对新手**：显著提升效率，快速补齐技能短板
- **对高手**：在创新场景中可能成为阻碍，生成的「标准答案」可能限制思维

---

### 3. 代码解释与文档生成

AI 在理解代码并生成人类可读内容方面表现优异：

- **代码注释**：为函数、类、复杂逻辑添加解释性注释
- **API 文档**：生成接口说明、参数描述、返回值文档
- **README 编写**：项目介绍、安装指南、使用示例
- **代码走读**：解释陌生代码的逻辑和意图

---

### 4. 样板代码与配置生成

对于重复性高、模式固定的代码，AI 可以快速生成：

- **项目脚手架**：初始化项目结构、配置文件
- **配置文件**：Dockerfile、docker-compose、CI/CD pipeline
- **ORM 模型**：根据数据库 schema 生成模型类
- **接口定义**：根据需求生成 API 路由和处理框架

---

### 5. 代码重构辅助

AI 擅长识别和改进代码结构：

- **提取重复代码**：识别相似代码块，抽取公共函数
- **简化复杂表达式**：将嵌套条件改写为更清晰的形式
- **命名优化**：建议更具描述性的变量/函数名
- **代码现代化**：将旧语法升级为现代写法

---

### 6. Debug 辅助与错误分析

AI 在分析错误和定位问题方面有独特优势：

- **错误信息解读**：解释 stack trace、编译错误的含义
- **常见问题识别**：识别空指针、类型不匹配等典型错误
- **修复建议**：提供多种可能的解决方案
- **日志分析**：从大量日志中定位关键异常

---

### 7. 正则表达式与复杂语法

人类不擅长但 AI 表现出色的领域：

- **正则表达式**：编写和解释复杂的匹配模式
- **SQL 查询**：多表联查、窗口函数、复杂聚合
- **Shell 脚本**：命令组合、管道处理、参数解析
- **配置语法**：YAML、TOML、nginx.conf 等专用格式

---

### 8. 测试场景设计

当存在严格规范和清晰文档时，AI 可以设计出全面的测试用例：

- **边界条件测试**：空值、极大值、极小值
- **异常场景覆盖**：网络超时、权限不足、数据格式错误
- **组合场景**：多条件交叉验证
- **回归测试用例**：基于已知 bug 生成防护测试

---

## 四、最佳实践建议

> 详细内容请参阅 **[AI-编程最佳实践指南.md](./AI-编程最佳实践指南.md)**

### 核心要点

| 建议 | 说明 | 对应陷阱 | 审核方式 |
|------|------|----------|----------|
| **持久化关键规范** | 使用 RULES.md / OpenSpec 保持上下文 | 上下文丢失 | 人工 |
| **提供充足上下文** | 引用项目结构、技术栈、已有公共模块 | 缺乏全局观 | 人工 |
| **验证 API 真实性** | 查阅官方文档，编写单测验证接口可用性 | 幻觉虚构 | 人工+机器 |
| **检查版本时效** | 确认依赖版本和 API 是否为最新推荐 | 过时知识 | 人工+机器 |
| **明确编码规范** | 指定命名风格、格式规范、禁止事项 | 缺乏一致性 | 人工 |
| **最小化操作范围** | 聚焦单一模块/文件，避免上下文超限和误改无关代码 | 上下文丢失 | 人工 |
| **分步骤进行** | 复杂任务拆分为小步骤，每步检查确认后再继续 | 自由发挥 | 人工 |
| **遵循 KISS 原则** | 质疑过度抽象，要求解释必要性 | 过度工程化 | 人工 |
| **监控 Agent 执行** | 使用任务清单，分阶段验收，强调完成所有步骤 | Agent 执行不完整 | 人工 |
| **安全审查清单** | 检查注入、硬编码、权限问题 | 安全盲区 | 人工+机器 |
| **保持审查意识** | 重点检查一致性、重复代码、性能 | 综合 | 人工+机器 |

### RULES.md：项目级规范文档

`RULES.md` 是放置在项目根目录的规范文档，供 AI 助手在每次对话时引用，解决上下文丢失问题。

**建议包含：**
- 项目技术栈和架构概述
- 编码规范（命名、格式、注释）
- 公共模块位置和使用方式
- 禁止事项（如：禁止使用 `print` 调试）
- 安全要求

### 最小化操作范围：应对 Token 限制

由于 AI 的上下文窗口有限，一次性处理过多代码会导致：

| 问题 | 表现 |
|------|------|
| **上下文覆盖不全** | 文件开头的约束在处理末尾时被「遗忘」 |
| **误改无关代码** | AI 「顺手」修改了不在本次任务范围内的代码 |
| **理解偏差** | 信息过载导致 AI 对需求的理解出现偏差 |

**操作建议：**

```
❌ 错误做法：「帮我重构整个 src/ 目录」
✅ 正确做法：「帮我重构 src/utils/string.ts 中的 formatDate 函数」

❌ 错误做法：一次性提供 10 个文件让 AI 修改
✅ 正确做法：每次聚焦 1-2 个相关文件，完成后再处理下一批
```

**原则：**
- 每次任务聚焦**单一模块或单一功能点**
- 明确告知 AI **哪些文件/函数不要修改**
- 大型重构拆分为多个独立的小任务，逐步推进

### 快速参考

```
开始任务前 → 引用项目规范文档（RULES.md / OpenSpec）
生成函数前 → 检查 utils/ 是否已有类似实现
API 调用后 → 验证方法/参数是否真实存在
依赖引入后 → 确认版本是否为最新稳定版
复杂功能时 → 拆分为：接口设计 → 实现 → 测试
代码审查时 → 重点检查一致性、重复代码、安全漏洞
```

### 审核分工：机器 vs 人工

#### 机器可自动化审核

| 审核类型 | 具体内容 | 推荐工具 |
|----------|----------|----------|
| **代码风格** | 命名规范、格式缩进、注释规范 | ESLint、Prettier、clang-format |
| **重复代码** | 检测相似代码块、重复函数 | SonarQube、CPD、jscpd |
| **安全漏洞** | SQL注入、XSS、硬编码密钥 | Bandit、Semgrep、Snyk |
| **依赖检查** | 过时版本、已知漏洞、许可证 | npm audit、Dependabot、OWASP |
| **API 验证** | 接口可用性、参数正确性 | 单元测试、集成测试 |
| **复杂度分析** | 圈复杂度、函数长度、嵌套深度 | SonarQube、CodeClimate |
| **类型检查** | 类型错误、空值引用 | TypeScript、mypy、pylint |

#### 仍需人工审核

| 审核类型 | 原因 |
|----------|------|
| **业务逻辑正确性** | AI 不理解业务意图和领域知识 |
| **架构合理性** | 需要全局视角和长期维护考量 |
| **抽象是否过度** | 需要权衡实际场景和团队能力 |
| **命名语义准确性** | 需要理解领域术语和上下文 |
| **性能瓶颈定位** | 需要结合运行环境和真实负载 |

### OpenSpec：规格驱动开发框架（可选）

> 项目地址：[github.com/Fission-AI/OpenSpec](https://github.com/Fission-AI/OpenSpec)

对于复杂功能开发，OpenSpec 提供了更完整的规格驱动方案：

```
openspec/changes/add-dark-mode/
├── proposal.md    # 为什么做、改什么
├── specs/         # 需求和场景规格
├── design.md      # 技术方案设计
└── tasks.md       # 实现任务清单
```
**适用场景：** 新功能开发、复杂重构、跨会话长期任务、团队协作

**不适用：** 简单 bug 修复、一次性脚本

### AI 全流程闭环：发挥最大效用的关键

> ⚠️ **核心洞察**：AI 编程要发挥最大作用，必须形成完整的闭环——设计 → 开发 → 测试 → 验证设计，全流程可由 AI 跑通。

#### 为什么需要闭环？

| 问题 | 没有闭环时 | 有闭环时 |
|------|------------|----------|
| **设计验证** | 设计文档写完就束之高阁，无法验证正确性 | 测试结果自动验证设计是否合理 |
| **AI 能力边界** | AI 只能做局部工作，缺乏反馈 | AI 可以根据测试反馈自我修正 |
| **质量保障** | 依赖人工全程把关，效率低 | 机器自动化验证，人工只需审核关键节点 |
| **迭代速度** | 每个环节都需要人工介入 | AI 可自主完成多轮迭代直到测试通过 |

#### 闭环工作流

```
┌─────────────────────────────────────────────────────────────┐
│                      AI 全流程闭环                          │
│                                                             │
│   ┌──────────┐     ┌──────────┐     ┌──────────┐           │
│   │  设计    │ ──▶ │  开发    │ ──▶ │  测试    │           │
│   │ (Design) │     │  (Code)  │     │  (Test)  │           │
│   └──────────┘     └──────────┘     └──────────┘           │
│        ▲                                  │                 │
│        │           ┌──────────┐           │                 │
│        └────────── │ 验证设计 │ ◀─────────┘                 │
│                    │ (Verify) │                             │
│                    └──────────┘                             │
│                                                             │
│   设计文档 ──▶ 生成代码 ──▶ 生成测试 ──▶ 执行验证 ──▶ 反馈设计  │
└─────────────────────────────────────────────────────────────┘
```

#### 各环节的 AI 职责

| 环节 | AI 职责 | 输入 | 输出 |
|------|---------|------|------|
| **设计** | 根据需求编写 OpenSpec 设计文档 | 需求描述、项目上下文 | `design.md`、接口定义 |
| **开发** | 根据设计文档生成实现代码 | 设计文档、编码规范 | 功能代码、模块实现 |
| **测试** | 根据设计文档生成测试用例并执行 | 设计文档、代码实现 | 测试代码、测试结果 |
| **验证** | 对比测试结果与设计预期，生成差异报告 | 测试结果、设计文档 | 验证报告、修改建议 |

#### 闭环的关键：设计文档必须可测试

**设计文档不仅是给人看的，更是给 AI 用来生成测试的依据。**

```markdown
## 设计文档示例（可测试的设计）

### 接口定义
ParseCSS(input: string) -> Result<StyleSheet, ParseError>

### 行为规格（可直接转化为测试用例）
1. 输入有效的 CSS 字符串，返回解析后的 StyleSheet 对象
2. 输入空字符串，返回空的 StyleSheet
3. 输入非法 CSS，返回 ParseError 且包含错误位置信息
4. 支持 CSS 注释，注释内容被忽略
5. 解析速度：10KB CSS 文件 < 100ms

### 边界条件
- 嵌套深度超过 100 层时返回错误
- 单条规则超过 10000 字符时截断并警告
```

#### 闭环实践流程

```
1. 编写可测试的设计文档（OpenSpec 格式）
       ↓
2. 设计文档经架构师审核（参考 Google-CPP-软件架构师角色定义.md）
       ↓
3. AI 根据设计文档生成实现代码
       ↓
4. AI 根据设计文档中的「行为规格」生成测试用例
       ↓
5. 执行测试，收集结果
       ↓
   ┌───────────────────────────────┐
   │ 测试通过？                     │
   │   ├── 是 → 提交代码审查        │
   │   └── 否 → AI 分析失败原因     │
   │            ├── 代码问题 → 修改代码，回到步骤 5     │
   │            └── 设计问题 → 修改设计，回到步骤 2     │
   └───────────────────────────────┘
```

#### 闭环的价值

| 价值 | 说明 |
|------|------|
| **设计质量提升** | 设计必须足够清晰才能生成测试，倒逼设计文档质量 |
| **早期发现问题** | 设计阶段就能通过测试用例发现逻辑漏洞 |
| **AI 自主迭代** | 测试失败时 AI 可自动修复，减少人工介入 |
| **知识沉淀** | 设计文档 + 测试用例 = 完整的功能规格，便于后续维护 |

#### 闭环工具链：Skill + Agent 平台

要实现完整的 AI 闭环，需要借助合适的工具链：

**1. Cursor Skills：定义闭环中的专项能力**

Skill 是 Cursor 中的 Agent 技能定义，可以为闭环的每个环节创建专项 Skill：

| 环节 | Skill 名称 | 职责 |
|------|-----------|------|
| 设计 | `design-spec-writer` | 根据需求生成 OpenSpec 格式的设计文档 |
| 开发 | `code-generator` | 根据设计文档生成符合规范的实现代码 |
| 测试 | `test-generator` | 根据设计文档的行为规格生成测试用例 |
| 验证 | `design-validator` | 对比测试结果与设计预期，生成差异报告 |

**Skill 示例结构（`SKILL.md`）**：

```markdown
# Design Spec Writer Skill

## 触发条件
当用户请求为新功能编写设计文档时激活

## 输入要求
- 功能需求描述
- 项目上下文（RULES.md）
- 相关模块接口

## 输出规范
- 生成 OpenSpec 格式的设计文档
- 包含可测试的行为规格
- 列出边界条件和错误处理

## 约束
- 设计文档必须经架构师审核（参考 Google-CPP-软件架构师角色定义.md）
- 行为规格必须可直接转化为测试用例
```

**2. OpenClaw：跨平台 Agent 执行引擎**

> 项目地址：[openclaw.ai](https://openclaw.ai/blog/introducing-openclaw)

[OpenClaw](https://openclaw.ai/blog/introducing-openclaw) 是一个开源的 Agent 平台，可以让 AI 助手在本地机器上运行，并通过多种渠道（WhatsApp、Telegram、Discord、Slack 等）触发执行。

**OpenClaw 在闭环中的作用**：

| 能力 | 闭环应用 |
|------|----------|
| **本地执行** | AI 可直接在开发机上执行构建、测试命令 |
| **多渠道触发** | 通过 Slack/Discord 触发 CI 流程，接收测试结果通知 |
| **持续运行** | 后台持续监控，测试失败时自动触发修复流程 |
| **数据本地化** | 代码和设计文档不离开本地，保证安全性 |

**闭环 + OpenClaw 工作流示例**：

```
┌─────────────────────────────────────────────────────────────────┐
│                  OpenClaw 驱动的 AI 闭环                         │
│                                                                 │
│  开发者 ──(Slack)──▶ OpenClaw Agent ──▶ 本地开发环境            │
│                           │                                     │
│                           ▼                                     │
│              ┌─────────────────────────┐                        │
│              │ 1. 读取需求，生成设计文档  │                        │
│              │ 2. 设计文档提交审核       │                        │
│              │ 3. 审核通过，生成代码     │                        │
│              │ 4. 生成测试用例并执行     │                        │
│              │ 5. 测试结果反馈          │                        │
│              └─────────────────────────┘                        │
│                           │                                     │
│                           ▼                                     │
│  开发者 ◀──(Slack 通知)── 测试通过/失败报告                       │
└─────────────────────────────────────────────────────────────────┘
```

**3. 组合使用：Skill + OpenClaw + OpenSpec**

```
OpenSpec（设计规范）     ──▶  定义「做什么」
     │
     ▼
Cursor Skill（能力定义） ──▶  定义「怎么做」
     │
     ▼
OpenClaw（执行引擎）     ──▶  实现「自动跑」
```

**实际应用场景**：

1. **开发者在 Slack 发送**：「为用户登录模块编写设计文档」
2. **OpenClaw 触发** `design-spec-writer` Skill
3. **Skill 生成** OpenSpec 格式的设计文档，提交至 `docs/specs/`
4. **架构师审核通过后**，OpenClaw 触发 `code-generator` Skill 生成代码
5. **代码生成后**，自动触发 `test-generator` Skill 生成测试并执行
6. **测试结果通过 Slack 通知开发者**，失败时 AI 自动分析原因并尝试修复

> 💡 **关键点**：Skill 定义了 AI 的专项能力，OpenClaw 提供了执行引擎和触发机制，OpenSpec 规范了设计文档格式——三者结合，实现真正的「AI 自动化软件工程」。

> 💡 **关键点**：没有闭环的 AI 编程只是「辅助打字」；有闭环的 AI 编程才是「自动化软件工程」。

---

## 五、总结

| 维度 | AI 编程助手的定位 |
|------|-------------------|
| **角色** | 高效的代码生成工具，而非架构师或创新者 |
| **优势** | 快速实现已知模式，提升编码效率 |
| **局限** | 缺乏全局观、一致性意识和创新能力 |
| **最佳用法** | 明确规范 + 充足上下文 + 人工审查 |

---

*AI 是工具，人是主导。理解其能力边界，方能善用之。*
